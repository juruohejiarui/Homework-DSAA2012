@misc{cantonese_corpus_repo,
author = {jasonleeubc},
title = {Cantopop-corpus},
howpublished = {\url{https://github.com/jasonleeubc/Cantopop-corpus}},
year = {2025},
note = {2025-10-10}
},
@inproceedings{cheng2025tonecraft,
  title={ToneCraft: Cantonese Lyrics Generation with Harmony of Tones and Pitches},
  author={Cheng, Junyu and Pan, Chang and Li, Shuangyin},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  pages={335--353},
  year={2025}
},
@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
},
@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
},
@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}


@inproceedings{sheng_songmass_2020,
	title = {{SongMASS}: {Automatic} {Song} {Writing} with {Pre}-training and {Alignment} {Constraint}},
	url = {https://www.microsoft.com/en-us/research/publication/songmass-automatic-song-writing-with-pre-training-and-alignment-constraint/},
	abstract = {Automatic song writing aims to compose a song (lyric and/or melody) by machine, which is an interesting topic in both academia and industry. In automatic song writing, lyric-to-melody generation and melody-to-lyric generation are two important tasks, both of which usually suffer from the following challenges: 1) the paired lyric and melody data are limited, which affects the generation quality of the two tasks, considering a lot of paired training data are needed due to the weak correlation between lyric and melody; 2) Strict alignments are required between lyric and melody, which relies on specific alignment modeling. In this paper, we propose SongMASS to address the above challenges, which leverages masked sequence to sequence (MASS) pre-training and attention based alignment modeling for lyric-to-melody and melody-to-lyric generation. Specifically, 1) we extend the original sentence-level MASS pre-training to song level to better capture long contextual information in music, and use a separate encoder and decoder for each modality (lyric or melody); 2) we leverage sentence-level attention mask and token-level attention constraint during training to enhance the alignment between lyric and melody. During inference, we use a dynamic programming strategy to obtain the alignment between each word/syllable in lyric and note in melody. We pre-train SongMASS on unpaired lyric and melody datasets, and both objective and subjective evaluations demonstrate that SongMASS generates lyric and melody with significantly better quality than the baseline method without pre-training or alignment constraint.},
	booktitle = {{AAAI} 2021},
	author = {Sheng, Zhonghao and Song, Kaitao and Tan, Xu and Ren, Yi and Ye, Wei and Zhang, Shikun and Qin, Tao},
	month = dec,
	year = {2020},
}

@inproceedings{lu_syllable-structured_2019,
	title = {A syllable-structured, contextually-based conditionally generation of chinese lyrics},
	booktitle = {Pacific {Rim} {International} {Conference} on {Artificial} {Intelligence}},
	publisher = {Springer},
	author = {Lu, Xu and Wang, Jie and Zhuang, Bojin and Wang, Shaojun and Xiao, Jing},
	year = {2019},
	pages = {257--265},
}

@article{li_singing_2016,
	title = {Singing tones in {Cantonese} operas and pop songs},
	journal = {Speech Prosody 2016, May 31â€“June 3, Boston, USA},
	author = {Li, Bin and Choi, Chung-Nin},
	year = {2016},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}